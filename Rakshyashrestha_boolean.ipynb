{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from os import remove\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "STOPWORDS.remove('not')\n",
        "STOPWORDS.remove('or')\n",
        "STOPWORDS.remove('and')\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huhHzDCrJgR_",
        "outputId": "e28c292b-20a2-4df6-e48f-c6ee35aff9b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the documents\n",
        "def load_documents(directory):\n",
        "    documents = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(directory, filename), 'r') as f:\n",
        "                documents[filename] = f.read()\n",
        "    return documents\n",
        "\n",
        "documents = load_documents('Documents')"
      ],
      "metadata": {
        "id": "6ppSBz9YJhTG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning and preprocessing text\n",
        "def clean_text(text):\n",
        "\n",
        "  #Lowercasing the text\n",
        "  text = text.lower()\n",
        "\n",
        "  #Removing non-alphanumeric characters\n",
        "  text = re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "  #Tokenizing\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  #Removing stopwords\n",
        "  tokens = [token for token in tokens if token not in STOPWORDS]\n",
        "\n",
        "  #Lemmatizing\n",
        "  tokens = [LEMMATIZER.lemmatize(token) for token in tokens]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "#Cleaned document\n",
        "cleaned_documents = {filename: clean_text(text) for filename, text in documents.items()}"
      ],
      "metadata": {
        "id": "G0q6DUxlJhyd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inverted index\n",
        "def create_inverted_index(documents):\n",
        "    inverted_index = defaultdict(set)\n",
        "    for filename, tokens in documents.items():\n",
        "        for word in tokens:\n",
        "            inverted_index[word].add(filename)\n",
        "    return inverted_index\n",
        "\n",
        "#Creating inverted index\n",
        "inverted_index = create_inverted_index(cleaned_documents)"
      ],
      "metadata": {
        "id": "eoS6lteqSLLr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing all_documents with the set of all documents filenames\n",
        "all_documnets = set(documents.keys())\n",
        "\n",
        "#Function for 'AND' query\n",
        "def and_query(terms, inverted_index):\n",
        "  result - inverted_index.get(terms[0], set())\n",
        "  for term in terms[1:]:\n",
        "    result &= inverted_index.get(term, set())\n",
        "  return result"
      ],
      "metadata": {
        "id": "Q91H7JYLSza6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for 'OR' query\n",
        "def or_query(terms, inverted_index):\n",
        "  result = inverted_index.get(terms[0], set())\n",
        "  for term in terms[1:]:\n",
        "    result |= inverted_index.get(term, set())\n",
        "  return result"
      ],
      "metadata": {
        "id": "D4ewbvptTs6C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for 'NOT' query\n",
        "def not_query(term, inverted_index, all_documnets):\n",
        "  return all_documents - inverted_index.get(term, set())"
      ],
      "metadata": {
        "id": "bk4yjjJBULmK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting document to a list\n",
        "def convert_doc_ids_to_filenames(doc_ids):\n",
        "  return list(doc_ids)"
      ],
      "metadata": {
        "id": "RzfJYYPpUz2T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing the queries and boolean\n",
        "def process_query(query, inverted_index, all_documnets):\n",
        "\n",
        "  #Tokenize and preprocessing query\n",
        "  terms = [LEMMATIZER.lemmatize(term) for term in word_tokenize(query.lower()) if term not in STOPWORDS]\n",
        "  if 'and' in terms:\n",
        "    terms.remove('and')\n",
        "    result = and_query(terms, inverted_index) # Fixed: Removed extra = and aligned indentation\n",
        "  elif 'or' in terms:\n",
        "    terms.remove('or')\n",
        "    result = or_query(terms, inverted_index) # Fixed: Removed extra ` and aligned indentation\n",
        "  elif 'not' in terms:\n",
        "    terms.remove('not')\n",
        "    result = not_query(terms[0], inverted_index, all_documnets) # Fixed: Changed all_documents to all_documnets\n",
        "  else:\n",
        "    result = inverted_index.get(terms[0], set())\n",
        "  return convert_doc_ids_to_filenames(result)"
      ],
      "metadata": {
        "id": "1nzyhEemVUg8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example usage\n",
        "query = \"AI\"\n",
        "result = process_query(query, inverted_index, all_documnets)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVrgC9X5VU9S",
        "outputId": "60d09824-e0b7-4b35-b06e-b2929b6480e7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Doc2.txt', 'Doc4.txt', 'Doc1.txt']\n"
          ]
        }
      ]
    }
  ]
}